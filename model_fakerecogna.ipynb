{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9a05cc",
   "metadata": {},
   "source": [
    "# Treinamento e validação de Modelos - Dataset FakeRecogna\n",
    "Treinamento de um modelo usando como base o segundo dataset estudado: https://github.com/Gabriel-Lino-Garcia/FakeRecogna \n",
    "\n",
    "Após verificar que o primeiro modelo obteve uma performace ruim ao ser testado com outro dataset, fez-se a hipótese de que isto pode ter ocorrido devido à baixa variabilidade de notícias no primeiro dataset utilizado. Optou-se então por tentar treinar um modelo TF-IDF no segundo dataset, que possui maior variedade de notícias, para validar se isto resultaria em um modelo mais geral.\n",
    "\n",
    "Este modelo, além de ser validado no próprio dataset que o gerou, também é validado com o primeiro dataset utilizado no projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9537adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.2.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "     |████████████████████████████████| 435 kB 21.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Se necessário\n",
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f104f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from python_scripts.save_load import load_df_from_bucket, save_df_to_s3_bucket, save_to_s3_bucket_as_libsvm, BUCKET_MODEL\n",
    "from python_scripts.modelling import create_train_validation_test_sets, setup_model, make_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1504f",
   "metadata": {},
   "source": [
    "## Carregamento de dados\n",
    "O pré-processamento e padronização dos dados deste dataset já foi realizado no notebook `preprocessing.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545fb80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria</th>\n",
       "      <th>lemmas_str</th>\n",
       "      <th>fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entretenimento</td>\n",
       "      <td>apagão vaticano papar presar acusação tráfico ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saúde</td>\n",
       "      <td>governar equador anunciar preparar cova coleti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saúde</td>\n",
       "      <td>companhia air france operar voar direto pequim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saúde</td>\n",
       "      <td>marfrig global foods retomar vender carnar bov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entretenimento</td>\n",
       "      <td>assunto voltar compartilhar rede social julho ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Categoria                                         lemmas_str  fake\n",
       "0  entretenimento  apagão vaticano papar presar acusação tráfico ...     1\n",
       "1           saúde  governar equador anunciar preparar cova coleti...     0\n",
       "2           saúde  companhia air france operar voar direto pequim...     0\n",
       "3           saúde  marfrig global foods retomar vender carnar bov...     0\n",
       "4  entretenimento  assunto voltar compartilhar rede social julho ...     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = load_df_from_bucket('dados_processados_recogna.csv', tipo='processado')\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802c565",
   "metadata": {},
   "source": [
    "## Modelo 1: TF-IDF baseado no texto da notícia\n",
    "Vetorização TF-IDF é aplicada somente à coluna de texto da notícia. Demais colunas de dados não são consideradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2587bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1, validate_1 = create_train_validation_test_sets(model_df, \n",
    "                                                                stratify_col='fake',\n",
    "                                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242700dc",
   "metadata": {},
   "source": [
    "### Processamento adicional\n",
    "\n",
    "Um vetorizador TFIDF é utilizado para converter os dados textuais em colunas do DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c72d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(lowercase=False, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar vetorizador TFIDF e ajustar aos dados de treinamento\n",
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "tfidf.fit(train_1['lemmas_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc0a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y_1(base_df, tfidf, target_col='fake', lemma_col = 'lemmas_str'):\n",
    "    tfidf_res = tfidf.transform(base_df[lemma_col])\n",
    "    return tfidf_res, base_df[target_col]\n",
    "\n",
    "x_train_1, y_train_1 = create_x_y_1(train_1, tfidf)\n",
    "x_validate_1, y_validate_1 = create_x_y_1(validate_1, tfidf)\n",
    "x_test_1, y_test_1 = create_x_y_1(test_1, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43204aa",
   "metadata": {},
   "source": [
    "### Upload de dados para o S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f979e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "file_name_tuples = [(x_train_1, y_train_1, 'train'), \n",
    "                   (x_test_1, y_test_1, 'test'), \n",
    "                   (x_validate_1, y_validate_1, 'validate')]\n",
    "\n",
    "for x, y, prefix in file_name_tuples:\n",
    "    save_to_s3_bucket_as_libsvm(x, y, prefix=prefix, filename='model_1_2.libsvm', tipo='modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672bd51",
   "metadata": {},
   "source": [
    "### Treinar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5b9af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-14 20:32:07 Starting - Starting the training job...ProfilerReport-1668457927: InProgress\n",
      "...\n",
      "2022-11-14 20:32:43 Starting - Preparing the instances for training...............\n",
      "2022-11-14 20:35:22 Downloading - Downloading input data...\n",
      "2022-11-14 20:36:02 Training - Training image download completed. Training in progress.\u001b[34m[2022-11-14 20:35:59.983 ip-10-0-187-94.ec2.internal:8 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Distributed node training with 4 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4']\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:03:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2022-11-14 20:36:00.291 ip-10-0-161-58.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Distributed node training with 4 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4']\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] start listen on algo-1:9099\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9099}\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] No data received from connection ('10.0.161.58', 46216). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:00:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:03:INFO] No data received from connection ('10.0.187.94', 49260). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:06:INFO] No data received from connection ('10.0.185.208', 59724). Closing.\u001b[0m\n",
      "\u001b[35m[2022-11-14 20:36:06.772 ip-10-0-185-208.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] Distributed node training with 4 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4']\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:06:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[35m[20:36:12] task NULL got new rank 2\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:12:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:12:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] No data received from connection ('10.0.142.24', 55140). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] Recieve start signal from 10.0.142.24; assign rank 0\u001b[0m\n",
      "\u001b[34m[20:36:12] task NULL got new rank 1\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] Recieve start signal from 10.0.161.58; assign rank 1\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] Recieve start signal from 10.0.185.208; assign rank 2\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] Recieve start signal from 10.0.187.94; assign rank 3\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] @tracker All of 4 nodes getting started\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] @tracker 0.04513955116271973 secs between node start and job finish\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] start listen on algo-1:9100\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9100}\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:12:INFO] No data received from connection ('10.0.161.58', 38854). Closing.\u001b[0m\n",
      "\u001b[32m[20:36:12] task NULL got new rank 3\u001b[0m\n",
      "\u001b[32m[2022-11-14:20:36:12:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[32m[2022-11-14:20:36:12:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:15:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:15:INFO] No data received from connection ('10.0.187.94', 55984). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:15:INFO] No data received from connection ('10.0.142.24', 46792). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:15:INFO] No data received from connection ('10.0.185.208', 50346). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:15:INFO] Recieve start signal from 10.0.142.24; assign rank 0\u001b[0m\n",
      "\u001b[34m[20:36:15] task NULL got new rank 1\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:15:INFO] Recieve start signal from 10.0.161.58; assign rank 1\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:15:INFO] Recieve start signal from 10.0.185.208; assign rank 2\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:15:INFO] Train matrix has 9521 rows and 449038 columns\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:15:INFO] Validation matrix has 1191 rows\u001b[0m\n",
      "\u001b[34m[2022-11-14 20:36:15.945 ip-10-0-161-58.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:16:INFO] Recieve start signal from 10.0.187.94; assign rank 3\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:16:INFO] @tracker All of 4 nodes getting started\u001b[0m\n",
      "\u001b[32m[2022-11-14:20:36:15:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[32m[20:36:15] task NULL got new rank 3\u001b[0m\n",
      "\u001b[32m[2022-11-14:20:36:16:INFO] Train matrix has 9521 rows and 449038 columns\u001b[0m\n",
      "\u001b[32m[2022-11-14:20:36:16:INFO] Validation matrix has 1191 rows\u001b[0m\n",
      "\u001b[32m[2022-11-14 20:36:16.034 ip-10-0-187-94.ec2.internal:8 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[20:36:15] task NULL got new rank 2\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:16:INFO] Train matrix has 9521 rows and 449038 columns\u001b[0m\n",
      "\u001b[35m[2022-11-14:20:36:16:INFO] Validation matrix has 1191 rows\u001b[0m\n",
      "\u001b[35m[2022-11-14 20:36:16.033 ip-10-0-185-208.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[36m[2022-11-14 20:36:11.979 ip-10-0-142-24.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[36mReturning the value itself\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[36mReturning the value itself\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] Distributed node training with 4 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4']\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[36m[20:36:12] task NULL got new rank 0\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:12:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:15:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[36m[20:36:15] task NULL got new rank 0\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:16:INFO] Train matrix has 9521 rows and 449038 columns\u001b[0m\n",
      "\u001b[36m[2022-11-14:20:36:16:INFO] Validation matrix has 1191 rows\u001b[0m\n",
      "\u001b[36m[2022-11-14 20:36:16.033 ip-10-0-142-24.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:33:INFO] [0]#011train-auc:0.87005#011validation-auc:0.83754\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:36:46:INFO] [1]#011train-auc:0.90366#011validation-auc:0.87258\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:37:00:INFO] [2]#011train-auc:0.93405#011validation-auc:0.90916\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:37:14:INFO] [3]#011train-auc:0.94591#011validation-auc:0.92312\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:37:28:INFO] [4]#011train-auc:0.95555#011validation-auc:0.93017\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:37:40:INFO] [5]#011train-auc:0.95903#011validation-auc:0.93324\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:37:53:INFO] [6]#011train-auc:0.96354#011validation-auc:0.93745\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:38:04:INFO] [7]#011train-auc:0.96646#011validation-auc:0.93963\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:38:17:INFO] [8]#011train-auc:0.97017#011validation-auc:0.94412\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:38:31:INFO] [9]#011train-auc:0.97298#011validation-auc:0.94478\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:38:42:INFO] [10]#011train-auc:0.97551#011validation-auc:0.94595\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:38:53:INFO] [11]#011train-auc:0.97749#011validation-auc:0.94627\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:39:02:INFO] [12]#011train-auc:0.97926#011validation-auc:0.94985\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:39:15:INFO] [13]#011train-auc:0.98103#011validation-auc:0.95215\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:39:26:INFO] [14]#011train-auc:0.98228#011validation-auc:0.95426\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:39:35:INFO] [15]#011train-auc:0.98358#011validation-auc:0.95411\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:39:45:INFO] [16]#011train-auc:0.98452#011validation-auc:0.95421\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:39:56:INFO] [17]#011train-auc:0.98588#011validation-auc:0.95528\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:40:06:INFO] [18]#011train-auc:0.98675#011validation-auc:0.95547\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:40:17:INFO] [19]#011train-auc:0.98739#011validation-auc:0.95546\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:40:24:INFO] [20]#011train-auc:0.98795#011validation-auc:0.95534\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:40:33:INFO] [21]#011train-auc:0.98887#011validation-auc:0.95595\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:40:47:INFO] [22]#011train-auc:0.98980#011validation-auc:0.95758\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:40:57:INFO] [23]#011train-auc:0.99040#011validation-auc:0.95836\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:41:06:INFO] [24]#011train-auc:0.99108#011validation-auc:0.95918\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:41:17:INFO] [25]#011train-auc:0.99179#011validation-auc:0.95925\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:41:26:INFO] [26]#011train-auc:0.99214#011validation-auc:0.95925\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:41:34:INFO] [27]#011train-auc:0.99250#011validation-auc:0.95971\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:41:44:INFO] [28]#011train-auc:0.99297#011validation-auc:0.96085\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:41:55:INFO] [29]#011train-auc:0.99337#011validation-auc:0.96094\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:42:03:INFO] [30]#011train-auc:0.99359#011validation-auc:0.96170\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:42:11:INFO] [31]#011train-auc:0.99391#011validation-auc:0.96224\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:42:21:INFO] [32]#011train-auc:0.99426#011validation-auc:0.96274\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:42:31:INFO] [33]#011train-auc:0.99455#011validation-auc:0.96270\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:42:40:INFO] [34]#011train-auc:0.99483#011validation-auc:0.96325\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:42:48:INFO] [35]#011train-auc:0.99504#011validation-auc:0.96378\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:42:58:INFO] [36]#011train-auc:0.99536#011validation-auc:0.96411\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:43:10:INFO] [37]#011train-auc:0.99569#011validation-auc:0.96426\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:43:21:INFO] [38]#011train-auc:0.99596#011validation-auc:0.96460\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:43:31:INFO] [39]#011train-auc:0.99615#011validation-auc:0.96382\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:43:39:INFO] [40]#011train-auc:0.99637#011validation-auc:0.96404\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:43:48:INFO] [41]#011train-auc:0.99651#011validation-auc:0.96410\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:43:48:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34m[2022-11-14:20:43:48:INFO] @tracker 452.799076795578 secs between node start and job finish\u001b[0m\n",
      "\n",
      "2022-11-14 20:44:10 Uploading - Uploading generated training model\n",
      "2022-11-14 20:44:10 Completed - Training job completed\n",
      "Training seconds: 2104\n",
      "Billable seconds: 2104\n",
      "ready for hosting!\n"
     ]
    }
   ],
   "source": [
    "xgb_model_1, data_channels_1 = setup_model(base_image='xgboost', model_name='model_1_2', instance_count=4, \n",
    "                                           instance_type='ml.m4.xlarge')\n",
    "xgb_model_1.fit(inputs=data_channels_1)\n",
    "\n",
    "print('ready for hosting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0925db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor_1 = xgb_model_1.deploy(initial_instance_count=1,\n",
    "                                     serializer=sagemaker.serializers.LibSVMSerializer(),\n",
    "                                     instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de61ff",
   "metadata": {},
   "source": [
    "### Métricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa8015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = make_prediction(xgb_predictor_1, model_name='model_1_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c83d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89       595\n",
      "           1       0.86      0.93      0.90       595\n",
      "\n",
      "    accuracy                           0.89      1190\n",
      "   macro avg       0.90      0.89      0.89      1190\n",
      "weighted avg       0.90      0.89      0.89      1190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_1, y_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5e1b4",
   "metadata": {},
   "source": [
    "Verifica-se aqui um excelente desempenho mesmo com os dados separados para teste, como foi visto no notebook `model_fakebr.ipynb`para o dataset Fake.br-Corpus. Em seguida, será feita uma segunda validação utilizando este dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64e2fb",
   "metadata": {},
   "source": [
    "### Salvar predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_alt = pd.DataFrame({'pred_1_2': y_pred_1})1\n",
    "df_pred_alt.to_csv(f's3://{BUCKET_MODEL}/test/pred_1_2.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce890572",
   "metadata": {},
   "source": [
    "## Teste adicional - Dataset Fake.br-Corpus\n",
    "Serão amostradas 800 notícias do Dataset Fake.br-Corpus para uma segunda validação do presente modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b168cd",
   "metadata": {},
   "source": [
    "### Carregar dataset e amostrar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30021ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake</th>\n",
       "      <th>text</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>words_str</th>\n",
       "      <th>lemmas_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A divisão do STF ao meio entre partidários e ...</td>\n",
       "      <td>10.504673</td>\n",
       "      <td>6.785107</td>\n",
       "      <td>divisão STF meio partidários independentes fic...</td>\n",
       "      <td>divisão STF meio partidário independente ficar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>General manda recado para STF: \"Abaixaram as c...</td>\n",
       "      <td>10.866667</td>\n",
       "      <td>6.502610</td>\n",
       "      <td>General manda recado STF Abaixaram calças Cong...</td>\n",
       "      <td>general mandar recado STF abaixar calça congre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>O Nordeste acordou! Lula e o PT são enxotados:...</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.565873</td>\n",
       "      <td>Nordeste acordou Lula PT enxotados Chega bande...</td>\n",
       "      <td>nordeste acordar Lula PT enxotar chegar bandei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Dois relatórios da Polícia Federal, com análi...</td>\n",
       "      <td>16.878788</td>\n",
       "      <td>7.286668</td>\n",
       "      <td>Dois relatórios Polícia Federal análises mater...</td>\n",
       "      <td>dois relatório Polícia Federal análise materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Coreia do Norte declara status de QUASE-GUERRA...</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>6.531320</td>\n",
       "      <td>Coreia Norte declara status QUASE-GUERRA mobil...</td>\n",
       "      <td>Coreia Norte declarar status QUASE-GUERRA mobi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fake                                               text  avg_sent_len  \\\n",
       "0     0   A divisão do STF ao meio entre partidários e ...     10.504673   \n",
       "1     1  General manda recado para STF: \"Abaixaram as c...     10.866667   \n",
       "2     1  O Nordeste acordou! Lula e o PT são enxotados:...      7.333333   \n",
       "3     0   Dois relatórios da Polícia Federal, com análi...     16.878788   \n",
       "4     1  Coreia do Norte declara status de QUASE-GUERRA...     11.600000   \n",
       "\n",
       "   avg_word_len                                          words_str  \\\n",
       "0      6.785107  divisão STF meio partidários independentes fic...   \n",
       "1      6.502610  General manda recado STF Abaixaram calças Cong...   \n",
       "2      6.565873  Nordeste acordou Lula PT enxotados Chega bande...   \n",
       "3      7.286668  Dois relatórios Polícia Federal análises mater...   \n",
       "4      6.531320  Coreia Norte declara status QUASE-GUERRA mobil...   \n",
       "\n",
       "                                          lemmas_str  \n",
       "0  divisão STF meio partidário independente ficar...  \n",
       "1  general mandar recado STF abaixar calça congre...  \n",
       "2  nordeste acordar Lula PT enxotar chegar bandei...  \n",
       "3  dois relatório Polícia Federal análise materia...  \n",
       "4  Coreia Norte declarar status QUASE-GUERRA mobi...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternative_df = load_df_from_bucket('dados_processados.csv', tipo='processado')\n",
    "alternative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ebf5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake</th>\n",
       "      <th>lemmas_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>divisão STF meio partidário independente ficar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>general mandar recado STF abaixar calça congre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>nordeste acordar Lula PT enxotar chegar bandei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dois relatório Polícia Federal análise materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Coreia Norte declarar status QUASE-GUERRA mobi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fake                                         lemmas_str\n",
       "0     0  divisão STF meio partidário independente ficar...\n",
       "1     1  general mandar recado STF abaixar calça congre...\n",
       "2     1  nordeste acordar Lula PT enxotar chegar bandei...\n",
       "3     0  dois relatório Polícia Federal análise materia...\n",
       "4     1  Coreia Norte declarar status QUASE-GUERRA mobi..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternative_df = alternative_df[['fake', 'lemmas_str']]\n",
    "alternative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d481bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = alternative_df.sample(n=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd47706",
   "metadata": {},
   "source": [
    "#### Predição\n",
    "\n",
    "Aplicar TF-IDF para vetorizar, salvar dados no S3 e efetuar predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bff7e1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "x_test_alt, y_test_alt = create_x_y_1(sample_df, tfidf)\n",
    "save_to_s3_bucket_as_libsvm(x_test_alt, y_test_alt, \n",
    "                            prefix='test', filename='model_2_1_alt.libsvm', tipo='modelo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60185438",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_alt = make_prediction(xgb_predictor_1, model_name='model_2_1_alt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5ff17",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e387e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.94      0.77       381\n",
      "           1       0.91      0.52      0.66       419\n",
      "\n",
      "    accuracy                           0.72       800\n",
      "   macro avg       0.78      0.73      0.71       800\n",
      "weighted avg       0.78      0.72      0.71       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_alt, y_pred_alt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4ceb3",
   "metadata": {},
   "source": [
    "Os resultados neste modelo foram bem mais razoáveis. Embora o modelo obtido não obtenha resultados ideais ao avaliar as notícias do outro dataset, são resultados mais coerentes.\n",
    "\n",
    "Um modelo combinado, treinado baseado nos dois datasets, é apresentado no notebook `model_combinado.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8fa392",
   "metadata": {},
   "source": [
    "#### Salvar predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1de293b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_alt = pd.DataFrame({'pred_2_alt': y_pred_alt})\n",
    "df_pred_alt.to_csv(f's3://{BUCKET_MODEL}/test/pred_2_alt.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e090e",
   "metadata": {},
   "source": [
    "### Encerrar modelo\n",
    "Após pegar métricas e dados de interesse, encerrar modelo para evitar cobranças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eaef099",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_1.delete_endpoint(delete_endpoint_config=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
