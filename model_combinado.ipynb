{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3b7979",
   "metadata": {},
   "source": [
    "# Treinamento e validação de Modelos - Dataset Combinado\n",
    "Treinamento de um modelo usando como base um dataset combinado, constituído dos dois datasets utilizados ao longo do projeto: \n",
    "- Fake.br-Corpus: https://github.com/roneysco/Fake.br-Corpus\n",
    "- FakeRecogna: https://github.com/Gabriel-Lino-Garcia/FakeRecogna \n",
    "\n",
    "Espera-se que um modelo treinado a partir de ambos datasets possua um desempenho melhor em uma maior variedade de notícias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5377c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: wordcloud in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (1.8.2.2)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Se necessário\n",
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299ec8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from python_scripts.save_load import load_df_from_bucket, save_df_to_s3_bucket, save_to_s3_bucket_as_libsvm, BUCKET_MODEL\n",
    "from python_scripts.modelling import create_train_validation_test_sets, setup_model, make_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f77fd",
   "metadata": {},
   "source": [
    "## Carregamento de dados\n",
    "O dataset combinado já foi criado e salvo no notebook `preprocessing.ipynb`. Aqui, ele é apenas carregado novamente a partir do S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d301ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake</th>\n",
       "      <th>lemmas_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>divisão STF meio partidário independente ficar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>general mandar recado STF abaixar calça congre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>nordeste acordar Lula PT enxotar chegar bandei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dois relatório Polícia Federal análise materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Coreia Norte declarar status QUASE-GUERRA mobi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fake                                         lemmas_str\n",
       "0     0  divisão STF meio partidário independente ficar...\n",
       "1     1  general mandar recado STF abaixar calça congre...\n",
       "2     1  nordeste acordar Lula PT enxotar chegar bandei...\n",
       "3     0  dois relatório Polícia Federal análise materia...\n",
       "4     1  Coreia Norte declarar status QUASE-GUERRA mobi..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = load_df_from_bucket('dados_processados_combinados.csv', tipo='processado')\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e98e4",
   "metadata": {},
   "source": [
    "## Modelo 1: TF-IDF baseado no texto da notícia\n",
    "Vetorização TF-IDF é aplicada somente à coluna de texto da notícia. Demais colunas de dados não são consideradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c04834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanhos de teste e validação garantem que o arquivo de teste seja < 5 MB, facilita implementação\n",
    "train_1, test_1, validate_1 = create_train_validation_test_sets(model_df, \n",
    "                                                                stratify_col='fake',\n",
    "                                                                test_size=0.1, random_state=42,\n",
    "                                                                validation_size=0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863b31c",
   "metadata": {},
   "source": [
    "### Processamento adicional\n",
    "\n",
    "Um vetorizador TFIDF é utilizado para converter os dados textuais em colunas do DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba87514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(lowercase=False, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar vetorizador TFIDF e ajustar aos dados de treinamento\n",
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "tfidf.fit(train_1['lemmas_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c04cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y_1(base_df, tfidf, target_col='fake', lemma_col = 'lemmas_str'):\n",
    "    tfidf_res = tfidf.transform(base_df[lemma_col])\n",
    "    return tfidf_res, base_df[target_col]\n",
    "\n",
    "x_train_1, y_train_1 = create_x_y_1(train_1, tfidf)\n",
    "x_validate_1, y_validate_1 = create_x_y_1(validate_1, tfidf)\n",
    "x_test_1, y_test_1 = create_x_y_1(test_1, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362cd235",
   "metadata": {},
   "source": [
    "### Upload de dados para o S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2cf994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "file_name_tuples = [(x_train_1, y_train_1, 'train'), \n",
    "                   (x_test_1, y_test_1, 'test'), \n",
    "                   (x_validate_1, y_validate_1, 'validate')]\n",
    "\n",
    "for x, y, prefix in file_name_tuples:\n",
    "    save_to_s3_bucket_as_libsvm(x, y, prefix=prefix, filename='model_1_combo.libsvm', tipo='modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ce12a",
   "metadata": {},
   "source": [
    "### Treinar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e44535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-15 13:05:00 Starting - Starting the training job...ProfilerReport-1668517500: InProgress\n",
      "...\n",
      "2022-11-15 13:05:45 Starting - Preparing the instances for training.........\n",
      "2022-11-15 13:07:26 Downloading - Downloading input data......\n",
      "2022-11-15 13:08:26 Training - Downloading the training image.....\u001b[36m[2022-11-15 13:09:09.786 ip-10-0-109-253.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:09:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:09:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[36mReturning the value itself\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:09:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[36mReturning the value itself\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:09:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:09:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:09:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:10:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[35m[2022-11-15 13:09:09.568 ip-10-0-113-243.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:09:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:09:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:09:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:09:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:09:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:09:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:10:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:10:INFO] Distributed node training with 4 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4']\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:10:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:10:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:10:INFO] Distributed node training with 4 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4']\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:10:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[32m[2022-11-15 13:09:10.127 ip-10-0-122-18.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[32mReturning the value itself\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[32mReturning the value itself\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] Distributed node training with 4 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4']\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:10:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2022-11-15 13:09:09.627 ip-10-0-83-192.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:09:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:09:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:09:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:09:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:09:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:09:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:10:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:10:INFO] Distributed node training with 4 hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4']\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:10:INFO] start listen on algo-1:9099\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:10:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9099}\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:10:INFO] No data received from connection ('10.0.83.192', 49146). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:10:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:10:INFO] No data received from connection ('10.0.109.253', 39906). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:10:INFO] No data received from connection ('10.0.122.18', 34662). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] No data received from connection ('10.0.113.243', 38578). Closing.\u001b[0m\n",
      "\u001b[36m[13:09:13] task NULL got new rank 0\u001b[0m\n",
      "\u001b[32m[13:09:13] task NULL got new rank 2\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:13:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:13:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:13:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[35m[13:09:13] task NULL got new rank 1\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:13:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:13:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] Recieve start signal from 10.0.109.253; assign rank 0\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] Recieve start signal from 10.0.113.243; assign rank 1\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] Recieve start signal from 10.0.122.18; assign rank 2\u001b[0m\n",
      "\u001b[34m[13:09:13] task NULL got new rank 3\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] Recieve start signal from 10.0.83.192; assign rank 3\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] @tracker All of 4 nodes getting started\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] @tracker 0.04755091667175293 secs between node start and job finish\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] start listen on algo-1:9100\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9100}\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] No data received from connection ('10.0.83.192', 56786). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:13:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:13:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:13:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:16:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:16:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[35m[13:09:16] task NULL got new rank 1\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:16:INFO] Train matrix has 17191 rows and 1711421 columns\u001b[0m\n",
      "\u001b[35m[2022-11-15:13:09:16:INFO] Validation matrix has 1052 rows\u001b[0m\n",
      "\u001b[35m[2022-11-15 13:09:16.850 ip-10-0-113-243.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] No data received from connection ('10.0.109.253', 59568). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] No data received from connection ('10.0.122.18', 50958). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] No data received from connection ('10.0.113.243', 43254). Closing.\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] Recieve start signal from 10.0.109.253; assign rank 0\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] Recieve start signal from 10.0.113.243; assign rank 1\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] Recieve start signal from 10.0.122.18; assign rank 2\u001b[0m\n",
      "\u001b[34m[13:09:16] task NULL got new rank 3\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] Recieve start signal from 10.0.83.192; assign rank 3\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] @tracker All of 4 nodes getting started\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] Train matrix has 17191 rows and 1711421 columns\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:09:16:INFO] Validation matrix has 1052 rows\u001b[0m\n",
      "\u001b[34m[2022-11-15 13:09:16.941 ip-10-0-83-192.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:16:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[36m[13:09:16] task NULL got new rank 0\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:16:INFO] Train matrix has 17191 rows and 1711421 columns\u001b[0m\n",
      "\u001b[36m[2022-11-15:13:09:16:INFO] Validation matrix has 1052 rows\u001b[0m\n",
      "\u001b[36m[2022-11-15 13:09:16.936 ip-10-0-109-253.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[32m[13:09:16] task NULL got new rank 2\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:16:INFO] Train matrix has 17191 rows and 1711421 columns\u001b[0m\n",
      "\u001b[32m[2022-11-15:13:09:16:INFO] Validation matrix has 1052 rows\u001b[0m\n",
      "\u001b[32m[2022-11-15 13:09:16.940 ip-10-0-122-18.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\n",
      "2022-11-15 13:09:26 Training - Training image download completed. Training in progress.\u001b[34m[2022-11-15:13:10:44:INFO] [0]#011train-auc:0.86606#011validation-auc:0.82982\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:11:50:INFO] [1]#011train-auc:0.90344#011validation-auc:0.87285\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:13:00:INFO] [2]#011train-auc:0.92690#011validation-auc:0.89059\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:13:56:INFO] [3]#011train-auc:0.93969#011validation-auc:0.91727\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:14:58:INFO] [4]#011train-auc:0.94698#011validation-auc:0.92708\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:15:56:INFO] [5]#011train-auc:0.95342#011validation-auc:0.93176\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:16:52:INFO] [6]#011train-auc:0.95928#011validation-auc:0.93603\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:17:40:INFO] [7]#011train-auc:0.96421#011validation-auc:0.94482\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:18:44:INFO] [8]#011train-auc:0.96658#011validation-auc:0.94743\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:19:32:INFO] [9]#011train-auc:0.96915#011validation-auc:0.95018\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:20:26:INFO] [10]#011train-auc:0.97144#011validation-auc:0.95263\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:21:27:INFO] [11]#011train-auc:0.97311#011validation-auc:0.95457\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:22:13:INFO] [12]#011train-auc:0.97494#011validation-auc:0.95554\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:22:47:INFO] [13]#011train-auc:0.97594#011validation-auc:0.95628\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:23:43:INFO] [14]#011train-auc:0.97778#011validation-auc:0.95933\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:24:44:INFO] [15]#011train-auc:0.98029#011validation-auc:0.96305\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:25:35:INFO] [16]#011train-auc:0.98184#011validation-auc:0.96372\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:26:20:INFO] [17]#011train-auc:0.98269#011validation-auc:0.96414\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:27:06:INFO] [18]#011train-auc:0.98356#011validation-auc:0.96538\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:27:53:INFO] [19]#011train-auc:0.98463#011validation-auc:0.96651\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:28:31:INFO] [20]#011train-auc:0.98541#011validation-auc:0.96793\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:29:22:INFO] [21]#011train-auc:0.98620#011validation-auc:0.96806\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:30:09:INFO] [22]#011train-auc:0.98674#011validation-auc:0.96849\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:31:07:INFO] [23]#011train-auc:0.98762#011validation-auc:0.96936\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:31:50:INFO] [24]#011train-auc:0.98839#011validation-auc:0.96879\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:32:31:INFO] [25]#011train-auc:0.98890#011validation-auc:0.96877\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:33:18:INFO] [26]#011train-auc:0.98960#011validation-auc:0.96926\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:33:59:INFO] [27]#011train-auc:0.98997#011validation-auc:0.96915\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:34:42:INFO] [28]#011train-auc:0.99041#011validation-auc:0.97073\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:35:31:INFO] [29]#011train-auc:0.99092#011validation-auc:0.97159\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:36:18:INFO] [30]#011train-auc:0.99131#011validation-auc:0.97219\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:37:08:INFO] [31]#011train-auc:0.99166#011validation-auc:0.97263\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:37:41:INFO] [32]#011train-auc:0.99197#011validation-auc:0.97343\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:38:31:INFO] [33]#011train-auc:0.99244#011validation-auc:0.97428\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:39:07:INFO] [34]#011train-auc:0.99275#011validation-auc:0.97433\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:39:49:INFO] [35]#011train-auc:0.99299#011validation-auc:0.97486\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:40:43:INFO] [36]#011train-auc:0.99330#011validation-auc:0.97488\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:41:29:INFO] [37]#011train-auc:0.99367#011validation-auc:0.97537\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:42:02:INFO] [38]#011train-auc:0.99394#011validation-auc:0.97528\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:42:36:INFO] [39]#011train-auc:0.99421#011validation-auc:0.97459\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:43:16:INFO] [40]#011train-auc:0.99441#011validation-auc:0.97524\u001b[0m\n",
      "\n",
      "2022-11-15 13:44:01 Uploading - Uploading generated training model\u001b[34m[2022-11-15:13:43:57:INFO] [41]#011train-auc:0.99462#011validation-auc:0.97579\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:43:58:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[34m[2022-11-15:13:43:58:INFO] @tracker 2081.186984062195 secs between node start and job finish\u001b[0m\n",
      "\n",
      "2022-11-15 13:44:35 Completed - Training job completed\n",
      "ProfilerReport-1668517500: NoIssuesFound\n",
      "Training seconds: 8860\n",
      "Billable seconds: 8860\n",
      "ready for hosting!\n"
     ]
    }
   ],
   "source": [
    "xgb_model_1, data_channels_1 = setup_model(base_image='xgboost', model_name='model_1_combo', instance_count=4, \n",
    "                                           instance_type='ml.m4.xlarge')\n",
    "xgb_model_1.fit(inputs=data_channels_1)\n",
    "\n",
    "print('ready for hosting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce55840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor_1 = xgb_model_1.deploy(initial_instance_count=1,\n",
    "                                     serializer=sagemaker.serializers.LibSVMSerializer(),\n",
    "                                     instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17984cd3",
   "metadata": {},
   "source": [
    "### Métricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74436e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = make_prediction(xgb_predictor_1, model_name='model_1_combo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799803f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90       430\n",
      "           1       0.88      0.94      0.91       429\n",
      "\n",
      "    accuracy                           0.91       859\n",
      "   macro avg       0.91      0.91      0.91       859\n",
      "weighted avg       0.91      0.91      0.91       859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_1, y_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12bf5eb",
   "metadata": {},
   "source": [
    "Verifica-se aqui um excelente desempenho mesmo com os dados separados para teste, como foi visto no notebook `model_fakebr.ipynb`para o dataset Fake.br-Corpus. Em seguida, será feita uma segunda validação utilizando este dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28afa4a1",
   "metadata": {},
   "source": [
    "### Salvar predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e30026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_alt = pd.DataFrame({'pred_1_combo': y_pred_1})\n",
    "df_pred_alt.to_csv(f's3://{BUCKET_MODEL}/test/pred_1_combo.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09c13",
   "metadata": {},
   "source": [
    "### Encerrar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f356e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_1.delete_endpoint(delete_endpoint_config=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
