{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddca6d2f",
   "metadata": {},
   "source": [
    "# Treinamento e validação de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "114350fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from python_scripts.save_load import load_df_from_bucket, save_df_to_s3_bucket, save_sparse_vector_to_s3_bucket_as_libsvm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0b653",
   "metadata": {},
   "source": [
    "### Processamento adicional\n",
    "Para utilizar os dados obtidos para treinamento de um modelo, algumas etapas adicionais de processamento serão executadas:\n",
    "- Apenas as colunas de lemmas, tamanho médio de sentença e tamanho médio de palavra serão utilizadas no modelo\n",
    "- Separação de dados de treino, validação e teste\n",
    "- Transformação TF-IDF será aplicada à coluna de lemmas\n",
    "- Colunas de tamanhos de sentenças e palavras serão padronizadas (ie RobustScaler para evitar problemas com outliers)\n",
    "- Salvar arquivos referentes aos dados de treino, validação e teste após este processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7b1f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>words_str</th>\n",
       "      <th>lemmas_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A divisão do STF ao meio entre partidários e ...</td>\n",
       "      <td>['divisão', 'STF', 'meio', 'partidários', 'ind...</td>\n",
       "      <td>['divisão', 'STF', 'meio', 'partidário', 'inde...</td>\n",
       "      <td>10.747664</td>\n",
       "      <td>6.690641</td>\n",
       "      <td>divisão STF meio partidários independentes fic...</td>\n",
       "      <td>divisão STF meio partidário independente ficar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>General manda recado para STF: \"Abaixaram as c...</td>\n",
       "      <td>['General', 'manda', 'recado', 'STF', 'Abaixar...</td>\n",
       "      <td>['general', 'mandar', 'recado', 'STF', 'abaixa...</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.461584</td>\n",
       "      <td>General manda recado STF Abaixaram calças Cong...</td>\n",
       "      <td>general mandar recado STF abaixar calça congre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>O Nordeste acordou! Lula e o PT são enxotados:...</td>\n",
       "      <td>['Nordeste', 'acordou', 'Lula', 'PT', 'enxotad...</td>\n",
       "      <td>['nordeste', 'acordar', 'Lula', 'PT', 'enxotar...</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.565873</td>\n",
       "      <td>Nordeste acordou Lula PT enxotados Chega bande...</td>\n",
       "      <td>nordeste acordar Lula PT enxotar chegar bandei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Dois relatórios da Polícia Federal, com análi...</td>\n",
       "      <td>['Dois', 'relatórios', 'Polícia', 'Federal', '...</td>\n",
       "      <td>['dois', 'relatório', 'Polícia', 'Federal', 'a...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>7.237319</td>\n",
       "      <td>Dois relatórios Polícia Federal análises mater...</td>\n",
       "      <td>dois relatório Polícia Federal análise materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Coreia do Norte declara status de QUASE-GUERRA...</td>\n",
       "      <td>['Coreia', 'Norte', 'declara', 'status', 'QUAS...</td>\n",
       "      <td>['Coreia', 'Norte', 'declarar', 'status', 'QUA...</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>6.513799</td>\n",
       "      <td>Coreia Norte declara status QUASE-GUERRA mobil...</td>\n",
       "      <td>Coreia Norte declarar status QUASE-GUERRA mobi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fake                                               text  \\\n",
       "0     0   A divisão do STF ao meio entre partidários e ...   \n",
       "1     1  General manda recado para STF: \"Abaixaram as c...   \n",
       "2     1  O Nordeste acordou! Lula e o PT são enxotados:...   \n",
       "3     0   Dois relatórios da Polícia Federal, com análi...   \n",
       "4     1  Coreia do Norte declara status de QUASE-GUERRA...   \n",
       "\n",
       "                                               words  \\\n",
       "0  ['divisão', 'STF', 'meio', 'partidários', 'ind...   \n",
       "1  ['General', 'manda', 'recado', 'STF', 'Abaixar...   \n",
       "2  ['Nordeste', 'acordou', 'Lula', 'PT', 'enxotad...   \n",
       "3  ['Dois', 'relatórios', 'Polícia', 'Federal', '...   \n",
       "4  ['Coreia', 'Norte', 'declara', 'status', 'QUAS...   \n",
       "\n",
       "                                              lemmas  avg_sent_len  \\\n",
       "0  ['divisão', 'STF', 'meio', 'partidário', 'inde...     10.747664   \n",
       "1  ['general', 'mandar', 'recado', 'STF', 'abaixa...     11.000000   \n",
       "2  ['nordeste', 'acordar', 'Lula', 'PT', 'enxotar...      7.333333   \n",
       "3  ['dois', 'relatório', 'Polícia', 'Federal', 'a...     17.000000   \n",
       "4  ['Coreia', 'Norte', 'declarar', 'status', 'QUA...     11.666667   \n",
       "\n",
       "   avg_word_len                                          words_str  \\\n",
       "0      6.690641  divisão STF meio partidários independentes fic...   \n",
       "1      6.461584  General manda recado STF Abaixaram calças Cong...   \n",
       "2      6.565873  Nordeste acordou Lula PT enxotados Chega bande...   \n",
       "3      7.237319  Dois relatórios Polícia Federal análises mater...   \n",
       "4      6.513799  Coreia Norte declara status QUASE-GUERRA mobil...   \n",
       "\n",
       "                                          lemmas_str  \n",
       "0  divisão STF meio partidário independente ficar...  \n",
       "1  general mandar recado STF abaixar calça congre...  \n",
       "2  nordeste acordar Lula PT enxotar chegar bandei...  \n",
       "3  dois relatório Polícia Federal análise materia...  \n",
       "4  Coreia Norte declarar status QUASE-GUERRA mobi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = load_df_from_bucket('dados_processados.csv', tipo='processado')\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26be00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake</th>\n",
       "      <th>lemmas_str</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>avg_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>divisão STF meio partidário independente ficar...</td>\n",
       "      <td>10.747664</td>\n",
       "      <td>6.690641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>general mandar recado STF abaixar calça congre...</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.461584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>nordeste acordar Lula PT enxotar chegar bandei...</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6.565873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dois relatório Polícia Federal análise materia...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>7.237319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Coreia Norte declarar status QUASE-GUERRA mobi...</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>6.513799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fake                                         lemmas_str  avg_sent_len  \\\n",
       "0     0  divisão STF meio partidário independente ficar...     10.747664   \n",
       "1     1  general mandar recado STF abaixar calça congre...     11.000000   \n",
       "2     1  nordeste acordar Lula PT enxotar chegar bandei...      7.333333   \n",
       "3     0  dois relatório Polícia Federal análise materia...     17.000000   \n",
       "4     1  Coreia Norte declarar status QUASE-GUERRA mobi...     11.666667   \n",
       "\n",
       "   avg_word_len  \n",
       "0      6.690641  \n",
       "1      6.461584  \n",
       "2      6.565873  \n",
       "3      7.237319  \n",
       "4      6.513799  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = full_df[['fake', 'lemmas_str', 'avg_sent_len', 'avg_word_len']]\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08662826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_validate = train_test_split(model_df, test_size=0.2, random_state=42, stratify=model_df['fake'])\n",
    "test, validate = train_test_split(test_validate, test_size=0.2, random_state=42, stratify=test_validate['fake'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ffe28",
   "metadata": {},
   "source": [
    "### Processamento adicional\n",
    "\n",
    "Processamento adicional feito via ColumnTransformer do Scikit-learn, para que se aplique Scaling apenas às colunas numéricas e se aplique TFIDF apenas às colunas textuais. O transformador é fittado apenas nos dados de treinamento, mas a transformação é aplicada nos três conjuntos (treinamento, validação e teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f521258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(lowercase=False, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar vetorizador TFIDF e ajustar aos dados de treinamento\n",
    "tfidf = TfidfVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "tfidf.fit(train['lemmas_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20afa582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse(df, tfidf_model):\n",
    "    \"\"\"\n",
    "    Função auxiliar para criar matriz esparsa com a vectorização TF-IDF adicionada de outras\n",
    "    colunas relevantes de df (fake, avg_sent_len e avg_word_len)\n",
    "    Coluna 'fake' vem primeiro, padrão do SageMaker\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "        Enviar tfidf_model já treinado, aqui usa apenas transform!!\n",
    "    \"\"\"\n",
    "    sparse_vector = tfidf_model.transform(df['lemmas_str'])\n",
    "    df_no_lemmas = df.drop(['lemmas_str', 'fake'], axis=1)\n",
    "    df_as_sparse = csr_matrix(df_no_lemmas.values)\n",
    "    final_sparse = hstack([df_as_sparse, sparse_vector])\n",
    "#     return final_sparse, df['fake'].values\n",
    "    return df_as_sparse, df['fake'].values   # Debugando, usar só as colunas numéricas pra ficar levim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dd10a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = create_sparse(train, tfidf)\n",
    "x_validate, y_validate = create_sparse(validate, tfidf)\n",
    "x_test, y_test = create_sparse(test, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfb03f",
   "metadata": {},
   "source": [
    "### Salvar matrizes esparsas no S3 para utilizar nos modelos\n",
    "Pontos importantes para funcionar com treinamento do Sagemaker:\n",
    "- Coluna alvo ('fake') deve ser a primeira coluna do DataFrame (já está neste formato)\n",
    "- Salvar sem cabeçalhos nem índices (já está feito, são matrizes esparsas puras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccfa471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "sparse_name_tuples = [(x_train, y_train, 'train.libsvm'), \n",
    "                      (x_test, y_test, 'test.libsvm'), \n",
    "                      (x_validate, y_validate, 'validate.libsvm')]\n",
    "\n",
    "for x, y, file_name in sparse_name_tuples:\n",
    "    save_sparse_vector_to_s3_bucket_as_libsvm(x, y, file_name, tipo='modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779472a8",
   "metadata": {},
   "source": [
    "# Treinar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516e1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from python_scripts.save_load import BUCKET_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b817e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "\n",
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"auc\",\n",
    "             \"objective\": \"binary:logistic\"}\n",
    "\n",
    "s3_output_location=f\"s3://{BUCKET_MODEL}/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552b5991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-11 11:14:28 Starting - Starting the training job...ProfilerReport-1668165268: InProgress\n",
      "...\n",
      "2022-11-11 11:15:19 Starting - Preparing the instances for training.........\n",
      "2022-11-11 11:16:59 Downloading - Downloading input data...\n",
      "2022-11-11 11:17:25 Training - Downloading the training image...\n",
      "2022-11-11 11:18:00 Training - Training image download completed. Training in progress..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[11:17:55] 5760x2 matrix with 11520 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[11:17:55] 288x2 matrix with 576 entries loaded from /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[2022-11-11 11:17:55.908 ip-10-0-181-76.ec2.internal:1 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-11-11 11:17:55.909 ip-10-0-181-76.ec2.internal:1 INFO hook.py:151] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-11-11 11:17:55.909 ip-10-0-181-76.ec2.internal:1 INFO hook.py:196] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 5760 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 288 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.78402#011validation-auc:0.79548\u001b[0m\n",
      "\u001b[34m[2022-11-11 11:17:55.919 ip-10-0-181-76.ec2.internal:1 INFO hook.py:325] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.79281#011validation-auc:0.79757\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.79802#011validation-auc:0.79839\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.80314#011validation-auc:0.80719\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.80669#011validation-auc:0.81240\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.80939#011validation-auc:0.81457\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.81278#011validation-auc:0.81510\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.81473#011validation-auc:0.81600\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.81691#011validation-auc:0.81686\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.81852#011validation-auc:0.81783\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.82173#011validation-auc:0.82058\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.82353#011validation-auc:0.81807\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.82583#011validation-auc:0.81691\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.82800#011validation-auc:0.81771\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.82935#011validation-auc:0.81826\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.83050#011validation-auc:0.81872\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.83131#011validation-auc:0.81863\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.83444#011validation-auc:0.81781\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.83603#011validation-auc:0.82089\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.83974#011validation-auc:0.82186\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.83998#011validation-auc:0.82246\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.84215#011validation-auc:0.82205\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.84280#011validation-auc:0.82046\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.84344#011validation-auc:0.82055\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.84457#011validation-auc:0.82055\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.84676#011validation-auc:0.82058\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.85009#011validation-auc:0.82164\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.85191#011validation-auc:0.82345\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.85481#011validation-auc:0.82335\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.85569#011validation-auc:0.82398\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.85616#011validation-auc:0.82378\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.85838#011validation-auc:0.82542\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.86043#011validation-auc:0.82342\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:0.86106#011validation-auc:0.82164\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:0.86192#011validation-auc:0.82183\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:0.86219#011validation-auc:0.82178\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:0.86277#011validation-auc:0.82154\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:0.86351#011validation-auc:0.82145\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:0.86452#011validation-auc:0.82275\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:0.86824#011validation-auc:0.82364\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:0.87037#011validation-auc:0.82287\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:0.87438#011validation-auc:0.82521\u001b[0m\n",
      "\n",
      "2022-11-11 11:18:22 Uploading - Uploading generated training model\n",
      "2022-11-11 11:19:00 Completed - Training job completed\n",
      "ProfilerReport-1668165268: NoIssuesFound\n",
      "Training seconds: 94\n",
      "Billable seconds: 94\n",
      "ready for hosting!\n"
     ]
    }
   ],
   "source": [
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                        sagemaker.get_execution_role(),\n",
    "                                        instance_count=1,   # Mudar para algo maior que 1 no definitivo\n",
    "                                        instance_type='ml.m4.xlarge',\n",
    "                                        output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())\n",
    "\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(BUCKET_MODEL,'train','train.libsvm'), content_type='libsvm')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(BUCKET_MODEL,'validate','validate.libsvm'), content_type='libsvm')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}\n",
    "\n",
    "# Deu erro por diferença em nome de features!\n",
    "xgb_model.fit(inputs=data_channels)\n",
    "\n",
    "print('ready for hosting!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abb23849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "#tbm existe serializer=sagemaker.amazon.common.RecordSerializer(),\n",
    "\n",
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                                 serializer=sagemaker.serializers.LibSVMSerializer(),\n",
    "                                 instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b04e99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()\n",
    "serializer = sagemaker.serializers.LibSVMSerializer()\n",
    "s3_path = 's3://projetointerdisciplinartreinoteste/test/test.libsvm'\n",
    "\n",
    "with fs.open(s3_path) as libsvm_file:\n",
    "    y_pred = xgb_predictor.predict(libsvm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "375ab300",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = [1 if float(x) >= 0.5 else 0 for x in y_pred.decode('utf-8').split(',')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67370c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72       576\n",
      "           1       0.72      0.66      0.69       576\n",
      "\n",
      "    accuracy                           0.71      1152\n",
      "   macro avg       0.71      0.71      0.71      1152\n",
      "weighted avg       0.71      0.71      0.71      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcionou com csvserializer, mas não é o ideal...\n",
    "\n",
    "import io\n",
    "batch_X_csv_buffer = io.StringIO()\n",
    "test[['avg_sent_len', 'avg_word_len']].to_csv(batch_X_csv_buffer, header=False, index=False)\n",
    "test_row = batch_X_csv_buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ed6d11e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string argument expected, got 'bytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d95399a25600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwrite_spmatrix_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/amazon/common.py\u001b[0m in \u001b[0;36mwrite_spmatrix_to_sparse_tensor\u001b[0;34m(file, array, labels)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0m_write_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0m_write_recordio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/amazon/common.py\u001b[0m in \u001b[0;36m_write_recordio\u001b[0;34m(f, data)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \"\"\"\n\u001b[1;32m    259\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_kmagic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string argument expected, got 'bytes'"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.common import write_spmatrix_to_sparse_tensor\n",
    "import io\n",
    "buf = io.BytesIO()\n",
    "write_spmatrix_to_sparse_tensor(buf,x_test)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fd92fac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d65173df9a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3://projetointerdisciplinartreinoteste/test/test.io'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         request_args = self._create_request_args(\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         )\n\u001b[1;32m    161\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_create_request_args\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InferenceId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/amazon/common.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mserialized\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_predictor.predict('s3://projetointerdisciplinartreinoteste/test/test.io')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb7c847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)  # deletar para não ficar cobrando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c71c7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "dump_svmlight_file(x_train, y_train, 'train.libsvm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "290f0883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4696, 1023884)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc672931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4696,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e6548fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 1023884)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "503ea13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validate.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
